Logistic Regression Classification Report:
              precision    recall  f1-score   support

         acc       0.29      0.08      0.12        77
        good       0.00      0.00      0.00        14
       unacc       0.73      0.95      0.83       242
       vgood       0.20      0.15      0.17        13

    accuracy                           0.69       346
   macro avg       0.30      0.30      0.28       346
weighted avg       0.58      0.69      0.61       346


K-Nearest Neighbors Classification Report:
              precision    recall  f1-score   support

         acc       0.82      0.81      0.81        77
        good       0.88      0.50      0.64        14
       unacc       0.93      0.99      0.96       242
       vgood       1.00      0.38      0.56        13

    accuracy                           0.91       346
   macro avg       0.91      0.67      0.74       346
weighted avg       0.91      0.91      0.90       346


Decision Tree Classification Report:
              precision    recall  f1-score   support

         acc       0.95      0.99      0.97        77
        good       0.93      1.00      0.97        14
       unacc       1.00      0.99      1.00       242
       vgood       1.00      0.85      0.92        13

    accuracy                           0.99       346
   macro avg       0.97      0.96      0.96       346
weighted avg       0.99      0.99      0.99       346


Random Forest Classification Report:
              precision    recall  f1-score   support

         acc       0.95      0.94      0.94        77
        good       1.00      0.93      0.96        14
       unacc       0.98      0.99      0.99       242
       vgood       1.00      0.92      0.96        13

    accuracy                           0.97       346
   macro avg       0.98      0.94      0.96       346
weighted avg       0.97      0.97      0.97       346


Support Vector Machine Classification Report:
              precision    recall  f1-score   support

         acc       0.82      0.87      0.84        77
        good       0.80      0.29      0.42        14
       unacc       0.97      0.99      0.98       242
       vgood       0.91      0.77      0.83        13

    accuracy                           0.93       346
   macro avg       0.87      0.73      0.77       346
weighted avg       0.93      0.93      0.92       346


Gradient Boosting Classification Report:
              precision    recall  f1-score   support

         acc       0.93      0.99      0.96        77
        good       1.00      0.93      0.96        14
       unacc       1.00      0.98      0.99       242
       vgood       0.93      1.00      0.96        13

    accuracy                           0.98       346
   macro avg       0.96      0.97      0.97       346
weighted avg       0.98      0.98      0.98       346


--- Model Accuracy Comparison ---
Logistic Regression: 0.6908
K-Nearest Neighbors: 0.9075
Decision Tree: 0.9855
Random Forest: 0.9740
Support Vector Machine: 0.9277
Gradient Boosting: 0.9798

--- Cross-Validation Scores (Mean) ---
Logistic Regression: 0.6499 (+/- 0.0585)
K-Nearest Neighbors: 0.6604 (+/- 0.2123)
Decision Tree: 0.7941 (+/- 0.2167)
Random Forest: 0.8166 (+/- 0.0982)
Support Vector Machine: 0.7680 (+/- 0.1890)
Gradient Boosting: 0.7842 (+/- 0.1865)
Starting Grid Search...
Fitting 5 folds for each of 108 candidates, totalling 540 fits

Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}
Best Cross-Validation Score: 0.9645

Tuned Random Forest Test Accuracy: 0.9798

Tuned Model Classification Report:
              precision    recall  f1-score   support

         acc       0.97      0.94      0.95        77
        good       0.93      0.93      0.93        14
       unacc       0.98      1.00      0.99       242
       vgood       1.00      0.92      0.96        13

    accuracy                           0.98       346
   macro avg       0.97      0.95      0.96       346
weighted avg       0.98      0.98      0.98       346


Process finished with exit code 0

=============================================================================================================
the best Model is: *****Tuned Random Forest***** because the CV and the predictor are near to each other
=============================================================================================================